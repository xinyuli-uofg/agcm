<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Interpretable Concept-based Deep Learning Framework</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            margin: 0;
            padding: 0;
            background-color: #f9f9f9;
            color: #333;
            text-align: center;
        }
        .container {
            max-width: 900px;
            margin: 50px auto;
            background: white;
            padding: 20px;
            border-radius: 10px;
            box-shadow: 0px 4px 10px rgba(0, 0, 0, 0.1);
            text-align: left;
        }
        h1, h3 {
            text-align: center;
        }
        h1 {
            color: #0056b3;
        }
        h2 {
            color: #444;
            border-bottom: 2px solid #ddd;
            padding-bottom: 5px;
        }
        a {
            text-decoration: none;
            color: #007BFF;
            font-weight: bold;
        }
        a:hover {
            text-decoration: underline;
        }
        .abstract {
            text-align: justify;
        }
        .video-container {
            margin-top: 20px;
            text-align: center;
        }
        iframe {
            width: 100%;
            height: 400px;
            border-radius: 10px;
        }
        .code-section {
            background: #f4f4f4;
            padding: 15px;
            border-radius: 5px;
            text-align: left;
            font-family: monospace;
            overflow-x: auto;
        }
        .supplementary {
            background: #f8f8f8;
            padding: 15px;
            border-radius: 5px;
            margin-top: 20px;
            text-align: center;
        }
        footer {
            margin-top: 20px;
            font-size: 0.9em;
            color: #777;
            text-align: center;
        }
        .center-text {
        text-align: center;
        }
    </style>
</head>
<body>
    <div class="container">
        <h1>Interpretable Concept-based Deep Learning Framework</h1>
        <h1>for Multimodal Human Behavior Modeling</h1>
        <p class="center-text"><strong>Xinyu Li and Marwa Mahmoud</strong></p>
        <p class="center-text">School of Computing Science, University of Glasgow, UK</p>

        <h2>ðŸŽ¥ Video Demo</h2>
        <div class="video-container">
            <iframe src="https://www.youtube.com/embed/uB-AWsAG6xY" frameborder="0" allowfullscreen></iframe>
        </div>

        <h2>ðŸ“„ Abstract</h2>
        <div class="abstract">
            <p>
                In the contemporary era of intelligent connectivity, Affective Computing (AC), which enables systems to recognize, interpret, and respond to human behavior states, has become an integrated part of many AI systems.
                As one of the most critical components of responsible AI and trustworthiness in all human-centered systems, explainability has been a major concern in AC.
                Particularly, the recently released <strong>EU General Data Protection Regulation</strong> requires any high-risk AI systems to be sufficiently interpretable, including biometric-based systems and emotion recognition systems widely used in the affective computing field.
            </p>
            <p>
                Existing explainable methods often compromise between interpretability and performance.
                Most of them focus only on highlighting key network parameters without offering meaningful, domain-specific explanations to the stakeholders.
                Additionally, they also face challenges in effectively co-learning and explaining insights from multimodal data sources.
            </p>
            <p>
                To address these limitations, we propose a novel and generalizable framework, namely the <strong>Attention-Guided Concept Model (AGCM)</strong>, which provides learnable conceptual explanations by identifying <em>what concepts</em> lead to the predictions and <em>where</em> they are observed.
                AGCM is extendable to any spatial and temporal signals through multimodal concept alignment and co-learning, empowering stakeholders with deeper insights into the model's decision-making process.
            </p>
            <p>
                We validate the SOTA <strong>performance</strong> and human-interpretable <strong>explainability</strong> of AGCM on well-established Facial Expression Recognition benchmark datasets while also demonstrating its generalizability on more complex real-world human behavior understanding applications.
            </p>
        </div>

        <h2>ðŸ“„ Research Paper</h2>
        <p><a href="https://arxiv.org/abs/2502.10145" target="_blank">Read on arXiv</a></p>

        <h2>ðŸ“„ Supplementary File</h2>
        <div class="supplementary">
            <p><a href="supplementary.pdf" target="_blank">ðŸ“‚ Read the supplementary PDF for extra discussion</a></p>
        </div>

        <h2>ðŸ’» Code</h2>
        <p>To run the code, use the following commands or access our <a href="https://github.com/xinyuli-uofg/agcm" target="_blank">GitHub repository</a>:</p>
        <div class="code-section">
            <pre>
        git clone https://github.com/xinyuli-uofg/agcm
            </pre>
        </div>

        <h2>ðŸ“œ License</h2>
        <p>CC BY-NC 4.0 License</p>

        <footer>
            <p>Â© 2025 Xinyu Li & Marwa Mahmoud | Behaviour AI | University of Glasgow</p>
        </footer>
    </div>
</body>
</html>
